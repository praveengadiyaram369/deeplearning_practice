- Basic four steps involved in convolutional neural networks
        a) Convolution
        b) Pooling
        c) Flattening
        d) Full Connection

- convolution is a mathetical operation performed on two functions that produces a third function that expresses how the shape of one is modified by other. (f X g = h) - X is the convolutional function.

- f is an image and g is an feature detector and h is a feature map/projection. h is generated by applying convolution operation on f and g.

- convolution refers to both the result function and process of computing it.
    Ref: https://en.wikipedia.org/wiki/Convolution

- Before applying the convolution operation, we take a (filter/feature detector/kernel) which is a 3x3 grid. And we can imagine the image f as 128X128 grid of values.

- convolution process is nothing but moving 3x3 grid over the 128X128 grid and apply overlapping which simply multiplies the respective values and sum the output values.

        - Input Image  X  feature detector  =  feature map

- convolution operation with feature detectors help us reduce the size of the input images, with minimal loss of information.

- we apply different feature detectors to create different feature maps. We create many feature maps to obtain our first convolution layer.

- We need to break linearity in the output of convolutional layer, so we use a non linear activation function such as ReLU or leaky ReLU. 

        - Input image   ->      convolutional layer  ->    ReLU activation function

- Max pooling is a techique to reduce the size of feature map further by taking the max of values present inside a box.
    move the box all over the image randomly and take of the values.

    - feature map  ->  pooled feature map

- Flattening: it is an operation to transform the pooling output to a column data, which will be provided to ANN.

    - Input image -> Convolution operation -> ReLU activation -> Max pooling -> Flattening -> ANN

- CNN = input image -> convolutional layer -> pooling layer -> ANN

- During back propagation, not only just weights are updated, but also feature detectors are adjusted.

- Voting happens at the layer before output layer of the fully connected ANN, which is crucial for the classification.

- Softmax layers gives the probabilites of output layer neurons which always add up to one.

- Cross entropy function is much better compared to mean square error and classification error. Cross entropy uses logarithmic approach and magnifies the error. It is widely used in the classification tasks.

- convolutional and pooling layers breaks down the image into small pieces and fully connected NN layer takes the pieces and combines which pieces are together.

- working of pooling layer can be expressed as comparing the value in feature map to a threshold value.